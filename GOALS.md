# Project Goals

## Vision
Create a comprehensive, structured dataset (CSVs) derived from Wikipedia for the UK Series of "The Traitors" (Series 1, 2, 3, and 4). The system should scrape, parse, and organize data to provide deep insights into the show's dynamics, capturing "as much information as physically possible."

## Objectives
- **Data Completeness**: Capture all available data points including voting behaviors (round-by-round), candidate profiles, role changes (Traitor/Faithful), and demographics.
- **Structure**: Organize data into logical CSV files (e.g., `candidates.csv`, `votes.csv`, `episodes.csv`, `banishments.csv`) to maximize usability.
- **Scope**: Cover UK Series 1, 2, 3, and 4 (still in progress).

## Key Performance Indicators (KPIs)
- **Accuracy**: Data must match Wikipedia source tables exactly.
- **Granularity**: Voting data must be broken down by round/episode.
- **Coverage**: 100% of participants from all 4 series included.

## Milestones
1.  ‚úÖ **Scraper Prototype**: Successfully fetch and parse a single Wikipedia page (e.g., Series 1).
2.  ‚úÖ **Data Modeling**: Define the schema for CSV outputs to handle complex voting and demographic data.
3.  ‚úÖ **Series Implementation**:
    - ‚úÖ Scrape Series 1
    - ‚úÖ Scrape Series 2
    - ‚úÖ Scrape Series 3
    - ‚úÖ Scrape Series 4
4.  ‚úÖ **Data Export**: Robust CSV generation logic.
5.  ‚úÖ **Validation**: Verify data against source.
6.  ‚úÖ **Repository Storage**: Storage of CSVs into the repository for auditing and version control.
7.  üîÑ **Firebase Deployment**: Deploy API and data pipeline to Google Cloud Platform.
    - ‚è≥ Phase 0: Local testing with Firebase Emulator Suite and manual triggers
    - Firebase Hosting for static CSV files and CDN delivery
    - Cloud Functions for API endpoints
    - GitHub Actions for scheduled data ingestion (commits to repo)
    - Optional: Cloud Storage for backup/alternate data persistence 
